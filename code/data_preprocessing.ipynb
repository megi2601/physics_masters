{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "intreday_omega = r\"C:\\\\physics_masters\\\\data\\\\Bossa public market data\\\\pub\\\\intraday\\\\omega\"\n",
    "daily_omega = r\"C:\\\\physics_masters\\\\data\\\\Bossa public market data\\\\pub\\\\daily\\\\omega\"\n",
    "\n",
    "intraday_mstock = r\"C:\\\\physics_masters\\\\data\\\\Bossa public market data\\\\pub\\\\intraday\\\\mstock\"\n",
    "daily_mstock = r\"C:\\\\physics_masters\\\\data\\\\Bossa public market data\\\\pub\\\\daily\\\\mstock\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unzip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = r\"C:\\\\physics_masters\\\\data\\\\Bossa public market data\\\\pub\\\\intraday\\\\mstock\\\\fut\"\n",
    "output_dir = r\"C:\\\\physics_masters\\\\data\\\\Bossa public market data\\\\pub\\\\intraday_unzip\\\\mstock\"\n",
    "\n",
    "for filename in os.listdir(data_dir):\n",
    "    zip_path = os.path.join(data_dir, filename)\n",
    "    \n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# additional processing for mstock - remove second column, add header\n",
    "\n",
    "data_dir = r\"C:\\\\physics_masters\\\\data\\\\Bossa public market data\\\\pub\\\\intraday_unzip\\\\mstock\"\n",
    "output_dir = r\"C:\\\\physics_masters\\\\data\\\\Bossa public market data\\\\pub\\\\intraday_unzip\\\\mstock\"\n",
    "\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "header = [\"Name\", \"Date\", \"Time\", \"Open\", \"High\", \"Low\", \"Close\", \"Volume\", \"Oi\"]\n",
    "\n",
    "for filename in os.listdir(data_dir):\n",
    "    if filename.endswith(\".prn\"):\n",
    "        file_path = os.path.join(data_dir, filename)\n",
    "        df = pd.read_csv(file_path, delimiter=',', header=None)\n",
    "        df.drop(columns=[1], inplace=True)  # Remove the second column\n",
    "        df.to_csv(os.path.join(output_dir, filename), index=False, header=header)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create daily data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_daily_data(data_dir, output_dir):\n",
    "    dataframes = dict()\n",
    "\n",
    "    for filename in os.listdir(data_dir):\n",
    "        path = os.path.join(data_dir, filename)\n",
    "        df = pd.read_csv(path, delimiter=',', header=0)\n",
    "        #df.columns = [\"Name\", \"Date\", \"Time\", \"Open\", \"High\", \"Low\", \"Close\", \"Volume\", \"Oi\"]\n",
    "        #df[\"Date\"] = pd.to_datetime(df[\"Date\"])\n",
    "        # daily_diff = df.groupby(\"Date\").agg({\"Low\": \"first\", \"High\": \"last\"})\n",
    "        # daily_diff[\"DateDiff\"] = daily_diff[\"High\"] - daily_diff[\"Low\"]\n",
    "        #daily_diff = daily_diff[[\"DateDiff\"]]\n",
    "        \n",
    "        daily = df.groupby(\"Date\").agg({\"High\": \"last\"})\n",
    "        daily[\"DiffPrevDate\"] = daily[\"High\"].diff()\n",
    "        \n",
    "        # standarisation - so far without the 30 day std\n",
    "        daily[\"DiffPrevDate\"] = (daily[\"DiffPrevDate\"] - daily[\"DiffPrevDate\"].mean()) / daily[\"DiffPrevDate\"].std()\n",
    "        \n",
    "        output_file = os.path.join(output_dir, filename)\n",
    "        daily.to_csv(output_file)\n",
    "        dataframes[filename.replace('.prn', '')]=len(daily)\n",
    "    \n",
    "    df = pd.DataFrame(dataframes.items(), columns=['Asset', 'TotalDays'])\n",
    "    df.to_csv(rf'C:\\\\physics_masters\\\\data\\\\{output_dir.split(\"\\\\\")[-1]}_summary.txt', sep='\\t', index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_daily_data(intreday_omega, daily_omega)\n",
    "create_daily_data(intraday_mstock, daily_mstock)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data_dir in [daily_omega, daily_mstock]:\n",
    "    assets_info = []\n",
    "    for filename in os.listdir(data_dir):\n",
    "        path = os.path.join(data_dir, filename)\n",
    "        df = pd.read_csv(path, delimiter=',', header=0)\n",
    "\n",
    "        assets_info.append([filename.replace('.prn', '').ljust(10),  \n",
    "                            pd.to_datetime(df['Date'], format='%Y%m%d').min().strftime('%Y-%m-%d'), \n",
    "                            pd.to_datetime(df['Date'], format='%Y%m%d').max().strftime('%Y-%m-%d'),\n",
    "                            len(df),\n",
    "                            (pd.to_datetime(df['Date'], format='%Y%m%d').max() - pd.to_datetime(df['Date'], format='%Y%m%d').min()).days])\n",
    "\n",
    "    df = pd.DataFrame(assets_info, columns=['Asset',  'DateMin', 'DateMax', 'TotalDays','DateDiffDays'])\n",
    "    df.to_csv(rf'C:\\\\physics_masters\\\\data\\\\{data_dir.split(\"\\\\\")[-1]}_summary.txt', sep='\\t', index=False,)    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# others\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataframes(data_dir):\n",
    "    dataframes = dict()\n",
    "    for filename in os.listdir(data_dir):\n",
    "        path = os.path.join(data_dir, filename)\n",
    "        df = pd.read_csv(path, delimiter=',', header=0)\n",
    "        dataframes[filename.replace('.prn', '')] = df\n",
    "    return dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1 files\n",
      "Processed 2 files\n",
      "Processed 3 files\n",
      "Processed 4 files\n",
      "Processed 5 files\n",
      "Processed 6 files\n",
      "Processed 7 files\n",
      "Processed 8 files\n",
      "Processed 9 files\n",
      "Processed 10 files\n",
      "Processed 11 files\n",
      "Processed 12 files\n",
      "Processed 13 files\n",
      "Processed 14 files\n",
      "Processed 15 files\n",
      "Processed 16 files\n",
      "Processed 17 files\n",
      "Processed 18 files\n",
      "Processed 19 files\n",
      "Processed 20 files\n",
      "Processed 21 files\n",
      "Processed 22 files\n",
      "Processed 23 files\n",
      "Processed 24 files\n",
      "Processed 25 files\n",
      "Processed 26 files\n",
      "Processed 27 files\n",
      "Processed 28 files\n",
      "Processed 29 files\n",
      "Processed 30 files\n",
      "Processed 31 files\n",
      "Processed 32 files\n",
      "Processed 33 files\n",
      "Processed 34 files\n",
      "Processed 35 files\n",
      "Processed 36 files\n",
      "Processed 37 files\n",
      "Processed 38 files\n",
      "Processed 39 files\n",
      "Processed 40 files\n",
      "Processed 41 files\n",
      "Processed 42 files\n",
      "Processed 43 files\n",
      "Processed 44 files\n",
      "Processed 45 files\n",
      "Processed 46 files\n",
      "Processed 47 files\n",
      "Processed 48 files\n",
      "Processed 49 files\n",
      "Processed 50 files\n",
      "Processed 51 files\n",
      "Processed 52 files\n",
      "Processed 53 files\n",
      "Processed 54 files\n",
      "Processed 55 files\n",
      "Processed 56 files\n",
      "Processed 57 files\n",
      "Processed 58 files\n",
      "Processed 59 files\n",
      "Processed 60 files\n",
      "Processed 61 files\n",
      "Processed 62 files\n",
      "Processed 63 files\n",
      "Processed 64 files\n",
      "Processed 65 files\n",
      "Processed 66 files\n",
      "Processed 67 files\n",
      "Processed 68 files\n",
      "Processed 69 files\n",
      "Processed 70 files\n",
      "Processed 71 files\n",
      "Processed 72 files\n",
      "Processed 73 files\n",
      "Processed 74 files\n",
      "Processed 75 files\n",
      "Processed 76 files\n",
      "Processed 77 files\n",
      "Processed 78 files\n",
      "Processed 79 files\n",
      "Processed 80 files\n",
      "Processed 81 files\n",
      "Processed 82 files\n",
      "Processed 83 files\n",
      "Processed 84 files\n",
      "Processed 85 files\n",
      "Processed 86 files\n",
      "Processed 87 files\n",
      "Processed 88 files\n",
      "Processed 89 files\n",
      "Processed 90 files\n",
      "Processed 91 files\n",
      "Processed 92 files\n",
      "Processed 93 files\n",
      "Processed 94 files\n",
      "Processed 95 files\n",
      "Processed 96 files\n",
      "Processed 97 files\n",
      "Processed 98 files\n",
      "Processed 99 files\n",
      "Processed 100 files\n",
      "Processed 101 files\n",
      "Processed 102 files\n",
      "Processed 103 files\n",
      "Processed 104 files\n",
      "Processed 105 files\n",
      "Processed 106 files\n",
      "Processed 107 files\n",
      "Processed 108 files\n",
      "Processed 109 files\n",
      "Processed 110 files\n",
      "Processed 111 files\n",
      "Processed 112 files\n",
      "Processed 113 files\n",
      "Processed 114 files\n",
      "Processed 115 files\n",
      "Processed 116 files\n",
      "Processed 117 files\n",
      "Processed 118 files\n",
      "Processed 119 files\n",
      "Processed 120 files\n",
      "Processed 121 files\n",
      "Processed 122 files\n",
      "Processed 123 files\n",
      "Processed 124 files\n",
      "Processed 125 files\n",
      "Processed 126 files\n",
      "Processed 127 files\n",
      "Processed 128 files\n",
      "Processed 129 files\n",
      "Processed 130 files\n",
      "Processed 131 files\n",
      "Processed 132 files\n",
      "Processed 133 files\n",
      "Processed 134 files\n",
      "Processed 135 files\n",
      "Processed 136 files\n",
      "Processed 137 files\n",
      "Processed 138 files\n",
      "Processed 139 files\n",
      "Processed 140 files\n",
      "Processed 141 files\n",
      "Processed 142 files\n",
      "Processed 143 files\n",
      "Processed 144 files\n",
      "Processed 145 files\n",
      "Processed 146 files\n",
      "Processed 147 files\n",
      "Processed 148 files\n",
      "Processed 149 files\n",
      "Processed 150 files\n",
      "Processed 151 files\n",
      "Processed 152 files\n",
      "Processed 153 files\n",
      "Processed 154 files\n",
      "Processed 155 files\n",
      "Processed 156 files\n",
      "Processed 157 files\n",
      "Processed 158 files\n",
      "Processed 159 files\n",
      "Processed 160 files\n",
      "Processed 161 files\n",
      "Processed 162 files\n",
      "Processed 163 files\n",
      "Processed 164 files\n",
      "Processed 165 files\n",
      "Processed 166 files\n",
      "Processed 167 files\n",
      "Processed 168 files\n",
      "Processed 169 files\n",
      "Processed 170 files\n",
      "Processed 171 files\n",
      "Processed 172 files\n",
      "Processed 173 files\n",
      "Processed 174 files\n",
      "Processed 175 files\n",
      "Processed 176 files\n",
      "Processed 177 files\n",
      "Processed 178 files\n",
      "Processed 179 files\n",
      "Processed 180 files\n",
      "Processed 181 files\n",
      "Processed 182 files\n",
      "Processed 183 files\n",
      "Processed 184 files\n",
      "Processed 185 files\n",
      "Processed 186 files\n",
      "Processed 187 files\n",
      "Processed 188 files\n",
      "Processed 189 files\n",
      "Processed 190 files\n",
      "Processed 191 files\n",
      "Processed 192 files\n",
      "Processed 193 files\n",
      "Processed 194 files\n",
      "Processed 195 files\n",
      "Processed 196 files\n",
      "Processed 197 files\n",
      "Processed 198 files\n",
      "Processed 199 files\n",
      "Processed 200 files\n",
      "Processed 201 files\n",
      "Processed 202 files\n",
      "Processed 203 files\n",
      "Processed 204 files\n",
      "Processed 205 files\n",
      "Processed 206 files\n",
      "Processed 207 files\n",
      "Processed 208 files\n",
      "Processed 209 files\n",
      "Processed 210 files\n",
      "Processed 211 files\n",
      "Processed 212 files\n",
      "Processed 213 files\n",
      "Processed 214 files\n",
      "Processed 215 files\n",
      "Processed 216 files\n",
      "Processed 217 files\n",
      "Processed 218 files\n",
      "Processed 219 files\n",
      "Processed 220 files\n",
      "Processed 221 files\n",
      "Processed 222 files\n",
      "Processed 223 files\n",
      "Processed 224 files\n",
      "Processed 225 files\n",
      "Processed 226 files\n",
      "Processed 227 files\n",
      "Processed 228 files\n",
      "Processed 229 files\n",
      "Processed 230 files\n",
      "Processed 231 files\n",
      "Processed 232 files\n",
      "Processed 233 files\n",
      "Processed 234 files\n",
      "Processed 235 files\n",
      "Processed 236 files\n",
      "Processed 237 files\n",
      "Processed 238 files\n",
      "Processed 239 files\n",
      "Processed 240 files\n",
      "Processed 241 files\n",
      "Processed 242 files\n",
      "Processed 243 files\n",
      "Processed 244 files\n",
      "Processed 245 files\n",
      "Processed 246 files\n",
      "Processed 247 files\n",
      "Processed 248 files\n",
      "Processed 249 files\n",
      "Processed 250 files\n",
      "Processed 251 files\n",
      "Processed 252 files\n",
      "Processed 253 files\n",
      "Processed 254 files\n",
      "Processed 255 files\n",
      "Processed 256 files\n",
      "Processed 257 files\n",
      "Processed 258 files\n",
      "Processed 259 files\n",
      "Processed 260 files\n",
      "Processed 261 files\n",
      "Processed 262 files\n",
      "Processed 263 files\n",
      "Processed 264 files\n",
      "Processed 265 files\n",
      "Processed 266 files\n",
      "Processed 267 files\n",
      "Processed 268 files\n",
      "Processed 269 files\n",
      "Processed 270 files\n",
      "Processed 271 files\n",
      "Processed 272 files\n",
      "Processed 273 files\n",
      "Processed 274 files\n",
      "Processed 275 files\n",
      "Processed 276 files\n",
      "Processed 277 files\n",
      "Processed 278 files\n",
      "Processed 279 files\n",
      "Processed 280 files\n",
      "Processed 281 files\n",
      "Processed 282 files\n",
      "Processed 283 files\n",
      "Processed 284 files\n",
      "Processed 285 files\n",
      "Processed 286 files\n",
      "Processed 287 files\n",
      "Processed 288 files\n",
      "Processed 289 files\n",
      "Processed 290 files\n",
      "Processed 291 files\n",
      "Processed 292 files\n",
      "Processed 293 files\n",
      "Processed 294 files\n",
      "Processed 295 files\n",
      "Processed 296 files\n",
      "Processed 297 files\n",
      "Processed 298 files\n",
      "Processed 299 files\n",
      "Processed 300 files\n",
      "Processed 301 files\n",
      "Processed 302 files\n",
      "Processed 303 files\n",
      "Processed 304 files\n",
      "Processed 305 files\n",
      "Processed 306 files\n",
      "Processed 307 files\n",
      "Processed 308 files\n",
      "Processed 309 files\n",
      "Processed 310 files\n",
      "Processed 311 files\n",
      "Processed 312 files\n",
      "Processed 313 files\n",
      "Processed 314 files\n",
      "Processed 315 files\n",
      "Processed 316 files\n",
      "Processed 317 files\n",
      "Processed 318 files\n",
      "Processed 319 files\n",
      "Processed 320 files\n",
      "Processed 321 files\n",
      "Processed 322 files\n",
      "Processed 323 files\n",
      "Processed 324 files\n",
      "Processed 325 files\n",
      "Processed 326 files\n",
      "Processed 327 files\n",
      "Processed 328 files\n",
      "Processed 329 files\n",
      "Processed 330 files\n",
      "Processed 331 files\n",
      "Processed 332 files\n",
      "Processed 333 files\n",
      "Processed 334 files\n",
      "Processed 335 files\n",
      "Processed 336 files\n",
      "Processed 337 files\n",
      "Processed 338 files\n",
      "Processed 339 files\n",
      "Processed 340 files\n",
      "Processed 341 files\n",
      "Processed 342 files\n",
      "Processed 343 files\n",
      "Processed 344 files\n",
      "Processed 345 files\n",
      "Processed 346 files\n",
      "Processed 347 files\n",
      "Processed 348 files\n",
      "Processed 349 files\n",
      "Processed 350 files\n",
      "Processed 351 files\n",
      "Processed 352 files\n",
      "Processed 353 files\n",
      "Processed 354 files\n",
      "Processed 355 files\n",
      "Processed 356 files\n",
      "Processed 357 files\n",
      "Processed 358 files\n",
      "Processed 359 files\n",
      "Processed 360 files\n",
      "Processed 361 files\n",
      "Processed 362 files\n",
      "Processed 363 files\n",
      "Processed 364 files\n",
      "Processed 365 files\n",
      "Processed 366 files\n",
      "Processed 367 files\n",
      "Processed 368 files\n",
      "Processed 369 files\n",
      "Processed 370 files\n",
      "Processed 371 files\n",
      "Processed 372 files\n",
      "Processed 373 files\n",
      "Processed 374 files\n",
      "Processed 375 files\n",
      "Processed 376 files\n",
      "Processed 377 files\n",
      "Processed 378 files\n",
      "Processed 379 files\n",
      "Processed 380 files\n",
      "Processed 381 files\n",
      "Processed 382 files\n",
      "Processed 383 files\n",
      "Processed 384 files\n",
      "Processed 385 files\n",
      "Processed 386 files\n",
      "Processed 387 files\n",
      "Processed 388 files\n",
      "Processed 389 files\n",
      "Processed 390 files\n",
      "Processed 391 files\n",
      "Processed 392 files\n",
      "Processed 393 files\n",
      "Processed 394 files\n",
      "Processed 395 files\n",
      "Processed 396 files\n",
      "Processed 397 files\n",
      "Processed 398 files\n",
      "Processed 399 files\n",
      "Processed 400 files\n",
      "Processed 401 files\n",
      "Processed 402 files\n",
      "Processed 403 files\n",
      "Processed 404 files\n",
      "Processed 405 files\n",
      "Processed 406 files\n",
      "Processed 407 files\n",
      "Processed 408 files\n",
      "Processed 409 files\n",
      "Processed 410 files\n",
      "Processed 411 files\n",
      "Processed 412 files\n",
      "Processed 413 files\n",
      "Processed 414 files\n",
      "Processed 415 files\n",
      "Processed 416 files\n",
      "Processed 417 files\n",
      "Processed 418 files\n",
      "Processed 419 files\n",
      "Processed 420 files\n",
      "Processed 421 files\n",
      "Processed 422 files\n",
      "Processed 423 files\n",
      "Processed 424 files\n",
      "Processed 425 files\n",
      "Processed 426 files\n",
      "Processed 427 files\n",
      "Processed 428 files\n",
      "Processed 429 files\n",
      "Processed 430 files\n",
      "Processed 431 files\n",
      "Processed 432 files\n",
      "Processed 433 files\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "data_dir = r\"C:\\\\physics_masters\\\\data\\\\Bossa public market data\\\\pub\\\\intraday\\\\omega\\\\cgl\"\n",
    "\n",
    "dataframes = []\n",
    "i = 0\n",
    "for filename in os.listdir(data_dir):\n",
    "    if filename.endswith(\".zip\"):\n",
    "        zip_path = os.path.join(data_dir, filename)\n",
    "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "            for file in zip_ref.namelist():\n",
    "                if file.endswith(\".prn\"):  # Ensure it is a .prn file\n",
    "                    with zip_ref.open(file) as f:\n",
    "                        # Read the data into a DataFrame\n",
    "                        df = pd.read_csv(f, delimiter=',', header=0)\n",
    "                        df.columns = [\"Name\", \"Date\", \"Time\", \"Open\", \"High\", \"Low\", \"Close\", \"Volume\", \"Oi\"]\n",
    "                        # df[\"Date\"] = pd.to_datetime(df[\"Date\"])\n",
    "                        df[\"Diff\"] = df[\"Close\"] - df[\"Open\"]\n",
    "                        df = df[[\"Date\", \"Time\", \"Diff\"]]\n",
    "                        df.rename(columns={\"Diff\": file.replace(\".prn\", \"\")}, inplace=True)\n",
    "                        dataframes.append(df)\n",
    "                        i += 1\n",
    "                        print(f\"Processed {i} files\")\n",
    "\n",
    "# Merge all DataFrames on Date and Time\n",
    "# result_df = dataframes[0]\n",
    "# for df in dataframes[1:]:\n",
    "#     result_df = pd.merge(result_df, df, on=[\"Date\", \"Time\"], how=\"outer\")\n",
    "\n",
    "# # Normalize each column (subtract mean and divide by std)\n",
    "# columns_to_normalize = result_df.columns.difference([\"Date\", \"Time\"])\n",
    "# result_df[columns_to_normalize] = result_df[columns_to_normalize].apply(lambda x: (x - x.mean()) / x.std())\n",
    "\n",
    "# # Compute R^T @ R\n",
    "# R = result_df[columns_to_normalize].to_numpy().T\n",
    "# cov_matrix = R @ R.T\n",
    "\n",
    "# # Plot the matrix\n",
    "# plt.imshow(cov_matrix, cmap='hot', interpolation='nearest')\n",
    "# plt.colorbar()\n",
    "# plt.title(\"R^T @ R Matrix\")\n",
    "# plt.show()\n",
    "\n",
    "# print(result_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = dataframes[0]\n",
    "for df in dataframes[1:30]:\n",
    "    result_df = pd.merge(result_df, df, on=[\"Date\", \"Time\"], how=\"outer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_normalize = result_df.columns.difference([\"Date\", \"Time\"])\n",
    "result_df[columns_to_normalize] = result_df[columns_to_normalize].apply(lambda x: (x - x.mean()) / x.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "R = result_df[columns_to_normalize].to_numpy().T\n",
    "R[np.isnan(R)] = 0  # Replace NaN with 0 for covariance calculation\n",
    "cov_matrix = R @ R.T"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
